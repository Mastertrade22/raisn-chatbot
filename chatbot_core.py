"""
Core Chatbot Module
Production-ready LangGraph chatbot that can be plugged into any application
"""

from typing import TypedDict, List, Literal
import json
from langgraph.graph import StateGraph, END

from llm_client import create_llm_client
from database import get_database_schema, execute_sql
from fuzzy_matching import get_fuzzy_matching_context
from config import (
    ROUTER_SYSTEM_PROMPT,
    SQL_GENERATOR_SYSTEM_PROMPT,
    RESPONSE_SYSTEM_PROMPT,
    GENERAL_CONVERSATION_PROMPT,
    ERROR_MESSAGES,
    MAX_SQL_RETRIES,
    RECENT_HISTORY_FOR_ROUTER
)


# =======================
# STATE DEFINITION
# =======================

class AgentState(TypedDict):
    """The shared memory of the graph"""
    question: str                # The User Query
    chat_history: List[dict]     # Past conversation (User: ..., AI: ...)
    query_type: str              # Router output: "factual", "semantic", "data"
    sql_query: str               # The SQL generated by the LLM
    sql_result: str              # The raw data retrieved from Query DB
    final_answer: str            # The final plain English response
    error: str                   # Tracks if SQL execution failed (for retries)
    retry_count: int             # Number of SQL retries
    model_name: str              # Which model is being used
    tenant_id: str               # Client/tenant ID for filtering (None means no filtering)


# =======================
# GRAPH NODES
# =======================

def router_node(state: AgentState) -> dict:
    """
    Node A: The Router
    Analyzes the question + chat_history to classify intent
    """
    try:
        llm = create_llm_client(state['model_name'])

        # Format chat history
        history_str = "\n".join([
            f"{msg['role']}: {msg['content']}"
            for msg in state.get('chat_history', [])[-RECENT_HISTORY_FOR_ROUTER:]
        ])

        prompt = f"""
Recent conversation:
{history_str if history_str else "None"}

Current question: {state['question']}

Classification:"""

        classification = llm.invoke(prompt, ROUTER_SYSTEM_PROMPT).strip().lower()

        # Extract just the classification word - default to "data" for everything except greetings
        if "general" in classification:
            query_type = "general"
        else:
            # Force everything else to be a data query
            query_type = "data"

        return {"query_type": query_type}

    except Exception as e:
        # Default to DATA query on error (not general)
        return {
            "query_type": "data",
            "sql_query": "",
            "sql_result": "",
            "error": f"Router error: {str(e)}"
        }


def sql_gen_node(state: AgentState) -> dict:
    """
    Node B: SQL Generator
    Generates SQL query based on the question
    """
    try:
        llm = create_llm_client(state['model_name'])

        # Get schema with tenant filtering if applicable
        tenant_id = state.get('tenant_id')
        schema = get_database_schema(tenant_id)

        # Get fuzzy matching context (available cities, projects, developers)
        fuzzy_context = get_fuzzy_matching_context(tenant_id)

        # Enhanced system prompt with schema and fuzzy matching context
        system_prompt_with_schema = f"""{SQL_GENERATOR_SYSTEM_PROMPT}

DATABASE SCHEMA:
{schema}
{fuzzy_context}"""

        # Build prompt with error feedback if this is a retry
        if state.get('error') and state.get('retry_count', 0) > 0:
            prompt = f"""User question: {state['question']}

Previous SQL attempt: {state.get('sql_query')}
Error received: {state.get('error')}

Please fix the SQL query based on the error above.

SQL query:"""
        else:
            prompt = f"""User question: {state['question']}

SQL query:"""

        sql_query = llm.invoke(prompt, system_prompt_with_schema).strip()

        # Clean up the SQL (remove markdown if present)
        sql_query = sql_query.replace("```sql", "").replace("```", "").strip()
        # Remove any remaining backticks
        sql_query = sql_query.replace("`", "")

        return {
            "sql_query": sql_query,
            "sql_result": "",  # Clear previous results
            "error": ""  # Clear previous errors
        }

    except Exception as e:
        return {
            "sql_query": "",  # Clear failed query
            "sql_result": "",  # Clear any old results
            "error": f"SQL generation failed: {str(e)}",
            "retry_count": state.get('retry_count', 0) + 1
        }


def execute_sql_node(state: AgentState) -> dict:
    """
    Node C: Query DB
    Executes the SQL query and handles errors
    """
    try:
        results, error = execute_sql(state['sql_query'])

        if error:
            # SQL execution failed
            retry_count = state.get('retry_count', 0) + 1
            return {
                "sql_result": "",
                "sql_query": state.get('sql_query', ''),
                "error": error,
                "retry_count": retry_count
            }

        # Success
        sql_result = json.dumps(results)
        return {
            "sql_result": sql_result,
            "sql_query": state['sql_query'],
            "error": "",
            "retry_count": 0
        }

    except Exception as e:
        retry_count = state.get('retry_count', 0) + 1
        return {
            "sql_result": "",
            "sql_query": state.get('sql_query', ''),
            "error": str(e),
            "retry_count": retry_count
        }


def response_node(state: AgentState) -> dict:
    """
    Node D: Response Synthesizer
    Generates the final natural language response
    """
    try:
        # Check if there were persistent errors
        if state.get('error') and state.get('retry_count', 0) >= MAX_SQL_RETRIES:
            # Maximum retries exceeded - provide user-friendly error message
            return {"final_answer": ERROR_MESSAGES["max_retries"]}

        llm = create_llm_client(state['model_name'])

        if state['query_type'] == 'data':
            # Validate we have both sql_query and sql_result
            sql_query = state.get('sql_query', '').strip()
            sql_result = state.get('sql_result', '').strip()

            if not sql_query or not sql_result:
                return {"final_answer": ERROR_MESSAGES["no_query_result"]}

            prompt = f"""User question: {state['question']}

SQL Query executed: {sql_query}
Results: {sql_result}

Provide a clear, concise answer:"""

            final_answer = llm.invoke(prompt, RESPONSE_SYSTEM_PROMPT)

        else:
            # General conversation (greetings only)
            final_answer = llm.invoke(state['question'], GENERAL_CONVERSATION_PROMPT)

        return {"final_answer": final_answer}

    except Exception as e:
        # Fallback response for any errors
        return {"final_answer": ERROR_MESSAGES["response_generation"]}


# =======================
# CONDITIONAL LOGIC
# =======================

def route_query(state: AgentState) -> Literal["sql_gen", "response"]:
    """Determines where to go after the router"""
    if state['query_type'] == 'data':
        return "sql_gen"
    else:
        return "response"


def check_sql_error(state: AgentState) -> Literal["sql_gen", "response"]:
    """Determines if we need to retry SQL generation"""
    if state.get('error') and state.get('retry_count', 0) < MAX_SQL_RETRIES:
        # Retry SQL generation
        return "sql_gen"
    else:
        return "response"


# =======================
# GRAPH CONSTRUCTION
# =======================

def create_chatbot_graph(model_id: str) -> StateGraph:
    """
    Creates the LangGraph workflow for the chatbot

    Args:
        model_id: The LLM model identifier to use

    Returns:
        StateGraph: Compiled LangGraph workflow
    """
    workflow = StateGraph(AgentState)

    # Add nodes
    workflow.add_node("router", router_node)
    workflow.add_node("sql_gen", sql_gen_node)
    workflow.add_node("execute_sql", execute_sql_node)
    workflow.add_node("response", response_node)

    # Set entry point
    workflow.set_entry_point("router")

    # Add conditional edge from router
    workflow.add_conditional_edges(
        "router",
        route_query,
        {
            "sql_gen": "sql_gen",
            "response": "response"
        }
    )

    # Add edge from SQL gen to execution
    workflow.add_edge("sql_gen", "execute_sql")

    # Add conditional edge for error handling (cyclic!)
    workflow.add_conditional_edges(
        "execute_sql",
        check_sql_error,
        {
            "sql_gen": "sql_gen",  # Retry SQL generation
            "response": "response"  # Move to response
        }
    )

    # Add edge from response to end
    workflow.add_edge("response", END)

    return workflow.compile()


# =======================
# PRODUCTION-READY CHATBOT CLASS
# =======================

class RealEstateChatbot:
    """
    Production-ready chatbot that can be plugged into any application
    """

    def __init__(self, model_id: str = "qwen/qwen-2.5-72b-instruct", tenant_id: str = None):
        """
        Initialize the chatbot

        Args:
            model_id: The LLM model identifier to use
            tenant_id: Optional tenant/client ID for filtering (None means no filtering)
        """
        self.model_id = model_id
        self.tenant_id = tenant_id
        self.graph = create_chatbot_graph(model_id)
        self.chat_history = []

    def ask(self, question: str, preserve_history: bool = True, tenant_id: str = None) -> dict:
        """
        Ask a question to the chatbot

        Args:
            question: User's question
            preserve_history: Whether to keep chat history for context
            tenant_id: Optional tenant ID override (uses instance tenant_id if not provided)

        Returns:
            dict: Response containing:
                - final_answer: The chatbot's response
                - query_type: Type of query ("data" or "general")
                - sql_query: SQL query if applicable
                - error: Error message if any
        """
        # Use provided tenant_id or fall back to instance tenant_id
        active_tenant_id = tenant_id if tenant_id is not None else self.tenant_id

        # Initial state
        initial_state = {
            "question": question,
            "chat_history": self.chat_history.copy(),
            "query_type": "",
            "sql_query": "",
            "sql_result": "",
            "final_answer": "",
            "error": "",
            "retry_count": 0,
            "model_name": self.model_id,
            "tenant_id": active_tenant_id
        }

        # Run the graph
        final_state = self.graph.invoke(initial_state)

        # Update chat history if preserving
        if preserve_history:
            self.chat_history.append({"role": "user", "content": question})
            self.chat_history.append({"role": "assistant", "content": final_state['final_answer']})

        # Return clean response
        return {
            "final_answer": final_state['final_answer'],
            "query_type": final_state['query_type'],
            "sql_query": final_state.get('sql_query', ''),
            "error": final_state.get('error', '')
        }

    def set_tenant(self, tenant_id: str = None):
        """
        Set the tenant/client ID for filtering

        Args:
            tenant_id: Tenant ID to filter by (None means no filtering)
        """
        self.tenant_id = tenant_id

    def reset_history(self):
        """Clear chat history"""
        self.chat_history = []

    def get_history(self) -> List[dict]:
        """Get current chat history"""
        return self.chat_history.copy()


# =======================
# CONVENIENCE FUNCTIONS
# =======================

def create_chatbot(model_id: str = "qwen/qwen-2.5-72b-instruct", tenant_id: str = None) -> RealEstateChatbot:
    """
    Factory function to create a chatbot instance

    Args:
        model_id: The LLM model identifier
        tenant_id: Optional tenant/client ID for filtering

    Returns:
        RealEstateChatbot: Ready-to-use chatbot instance
    """
    return RealEstateChatbot(model_id=model_id, tenant_id=tenant_id)
